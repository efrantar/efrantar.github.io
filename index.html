---
layout: home
title: Elias Frantar
subtitle: Machine Learning PhD Candidate @ IST Austria
---

<p>
I am a fourth-year Computer Science PhD candidate at IST Austria, supervised by Dan Alistarh, working on practical neural network compression.
Currently, I am particularly interested in quantizing and sparsifying very large models in order to make them more resource-efficient and accessible.
I have also been working on hardware-aware methods and in combining different forms of compression such as sparsity and quantization.
</p>
<p>
Previously, I completed my Computer Science bachelor's and master's at Vienna University of Technology for which I received the Austrian State Prize that is awarded to the top 0.3% of graduates in the country.
In my free time, I enjoy creating very fast Rubik's Cube solving robots, some of which have set new world-records and have gathered several million views on YouTube.
</p>

<h1>Publications</h1>

<p>
  <b>Scaling Laws for Sparsely-connected Foundation Models</b><br>
  Preprint 2023 - Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, Utku Evci<br>
  <a href="https://arxiv.org/abs/2309.08520">[arxiv]</a>
  <ul>
    <li>We determine the first scaling laws connecting parameter-sparsity, effective model size and amount of training data, in the context of modern Transformers trained on massive datasets.</li>
    <li>These laws characterize in which training settings which sparsity level is optimal and how much better sparse models can perform than dense ones.</li>
  </ul>
</p>

<p>
  <b>SparseGPT: Massive Language Models Can be Accurately Pruned in One-shot</b><br>
  Oral, ICML 2023 - Elias Frantar, Dan Alistarh<br>
  <a href="https://arxiv.org/abs/2301.00774">[arxiv]</a> <a href="https://github.com/IST-DASLab/sparsegpt">[github]</a>
  <ul>
    <li>We introduce the first pruning algorithm that is fast and accurate enough to successfuly impose non-trivial amounts of sparsity on 100+ billion parameter models.</li>
    <li>In addition to unstructured pruning, SparseGPT generalizes to the hardware-friendly 2:4 sparsity pattern and can be combined with quantization.</li>
  </ul>
</p>

<p>
  <b>GPTQ: Accurate Post-training Quantization for Generative Pretrained Transformers</b><br>
  ICLR 2023 - Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh<br>
  <a href="https://arxiv.org/abs/2210.17323">[arxiv]</a> <a href="https://github.com/IST-DASLab/gptq">[github]</a>
  <ul>
    <li>We develop the first quantization algorithm that is fast and accurate enough to successfully quantize 100+ billion parameter models to 4-bit and 3-bit precision.</li>
    <li>We demonstrate for the first time, via custom CUDA kernels, that vanilla weight-only quantization can bring substantial practical speedups for memory-bound generative inference.</li>
  </ul>
</p>

<ul>
  <li> Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh. GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers, Under Review. </li>
  <li> Denis Kuznedelev, Eldar Kurtic, Elias Frantar, Dan Alistarh. oViT: An Accurate Second-Order Pruning Framework for Vision Transformers, Under Review. </li>
  <li> Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, Dan Alistarh. The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models, EMNLP 2022.</li>
  <li> Elias Frantar, Dan Alistarh. Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning, NeurIPS 2022. </li>
  <li> Elias Frantar, Dan Alistarh. SPDY: Accurate Pruning with Speedup Guarantees, ICML 2022. </li>
  <li> Elias Frantar, Eldar Kurtic, Dan Alistarh. M-FAC: Efficient Matrix-Free Approximations of Second-Order Information, NeurIPS 2021. </li>
  <li> Nikola Konstantinov, Elias Frantar, Dan Alistarh, Christoph Lampert. On the Sample Complexity of Adversarial Multi-Source PAC Learning, ICML 2020. </li>
</ul>
