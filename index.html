---
layout: home
title: Elias Frantar
subtitle: Machine Learning PhD Candidate @ IST Austria
---

<p>
I am a fourth-year Computer Science PhD candidate at IST Austria, supervised by Dan Alistarh, working on practical neural network compression.
Currently, I am particularly interested in quantizing and sparsifying very large models in order to make them more resource-efficient and accessible.
I have also been working on hardware-aware methods and in combining different forms of compression such as sparsity and quantization.
</p>
<p>
Previously, I completed my Computer Science bachelor's and master's at Vienna University of Technology for which I received the Austrian State Prize that is awarded to the top 0.3% of graduates in the country.
In my free time, I enjoy creating very fast Rubik's Cube solving robots, some of which have set new world-records and have gathered several million views on YouTube.
</p>

<h1>Publications</h1>

<p>
  <b>Scaling Laws for Sparsely-connected Foundation Models, <a href="https://arxiv.org/abs/2309.08520">[arxiv]</a></b><br>
  Preprint 2023 - Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, Utku Evci
  <ul>
    <li>We determine the first scaling laws connecting parameter-sparsity, effective model size and amount of training data, in the context of modern Transformers trained on massive datasets.</li>
    <li>These laws characterize in which training settings which sparsity level is optimal and how much better sparse models can perform than dense ones.</li>
  </ul>
</p>

<ul>
  <li> Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh. GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers, Under Review. </li>
  <li> Denis Kuznedelev, Eldar Kurtic, Elias Frantar, Dan Alistarh. oViT: An Accurate Second-Order Pruning Framework for Vision Transformers, Under Review. </li>
  <li> Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, Dan Alistarh. The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models, EMNLP 2022.</li>
  <li> Elias Frantar, Dan Alistarh. Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning, NeurIPS 2022. </li>
  <li> Elias Frantar, Dan Alistarh. SPDY: Accurate Pruning with Speedup Guarantees, ICML 2022. </li>
  <li> Elias Frantar, Eldar Kurtic, Dan Alistarh. M-FAC: Efficient Matrix-Free Approximations of Second-Order Information, NeurIPS 2021. </li>
  <li> Nikola Konstantinov, Elias Frantar, Dan Alistarh, Christoph Lampert. On the Sample Complexity of Adversarial Multi-Source PAC Learning, ICML 2020. </li>
</ul>
