---
layout: home
title: Elias Frantar
subtitle: Machine Learning PhD Candidate @ IST Austria
---

<p>
I am a third-year Computer Science PhD candidate at IST Austria, supervised by Dan Alistarh, working on practical neural network compression.
Currently, I am particularly interested in quantizing and sparsifying very large models in order to make them more resource-efficient and accessible.
I have also been working on hardware-aware methods and in combining different forms of compression such as sparsity and quantization.
</p>
<p>
Previously, I completed my Computer Science bachelor's and master's at Vienna University of Technology for which I received the Austrian State Prize that is awarded to the top 0.3% of graduates in the country.
In my free time, I enjoy creating very fast Rubik's Cube solving robots, some of which have set new world-records and have gathered several million views on YouTube.
</p>
  
<h1>Publications</h1>

<ul>
  <li> Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh. Accurate Post-Training Quantization for Generative Pre-Trained Transformers, Under Review. </li>
  <li> Denis Kuznedelev, Eldar Kurtic, Elias Frantar, Dan Alistarh. oViT: An Accurate Second-Order Pruning Framework for Vision Transformers, Under Review. </li>
  <li> Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael Goin, Dan Alistarh. The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models, EMNLP 2022.</li>
  <li> Elias Frantar, Dan Alistarh. Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning, NeurIPS 2022. </li>
  <li> Elias Frantar, Dan Alistarh. SPDY: Accurate Pruning with Speedup Guarantees, ICML 2022. </li>
  <li> Elias Frantar, Eldar Kurtic, Dan Alistarh. M-FAC: Efficient Matrix-Free Approximations of Second-Order Information, NeurIPS 2021. </li>
  <li> Nikola Konstantinov, Elias Frantar, Dan Alistarh, Christoph Lampert. On the Sample Complexity of Adversarial Multi-Source PAC Learning, ICML 2020. </li>
</ul>
