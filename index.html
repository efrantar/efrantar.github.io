---
layout: home
title: Elias Frantar
subtitle: Machine Learning PhD Candidate @ IST Austria
---

<p>
I am a fourth-year Computer Science PhD candidate at IST Austria, supervised by Dan Alistarh, working on practical neural network compression.
Currently, I am particularly interested in quantizing and sparsifying very large models in order to make them more resource-efficient and accessible.
I have also been working on hardware-aware methods and in combining different forms of compression such as sparsity and quantization.
</p>
<p>
Previously, I completed my Computer Science bachelor's and master's at Vienna University of Technology for which I received the Austrian State Prize that is awarded to the top 0.3% of graduates in the country.
In my free time, I enjoy creating very fast Rubik's Cube solving robots, some of which have set new world-records and have gathered several million views on YouTube.
</p>

<h1>Selected Publications</h1>

<p>
  <b>Scaling Laws for Sparsely-connected Foundation Models</b><br>
  Preprint 2023 - Elias Frantar, Carlos Riquelme, Neil Houlsby, Dan Alistarh, Utku Evci<br>
  <a href="https://arxiv.org/abs/2309.08520">[arxiv]</a>
  <ul>
    <li>We determine the first scaling laws connecting parameter-sparsity, effective model size and amount of training data, in the context of modern Transformers trained on massive datasets.</li>
    <li>These laws characterize in which training settings which sparsity level is optimal and how much better sparse models can perform than dense ones.</li>
  </ul>
</p>

<p>
  <b>SparseGPT: Massive Language Models Can be Accurately Pruned in One-shot</b><br>
  Oral, ICML 2023 - Elias Frantar, Dan Alistarh<br>
  <a href="https://arxiv.org/abs/2301.00774">[arxiv]</a> <a href="https://github.com/IST-DASLab/sparsegpt">[github]</a>
  <ul>
    <li>We introduce the first pruning algorithm that is fast and accurate enough to successfuly impose non-trivial amounts of sparsity on 100+ billion parameter models.</li>
    <li>In addition to unstructured pruning, SparseGPT generalizes to the hardware-friendly 2:4 sparsity pattern and can be combined with quantization.</li>
  </ul>
</p>

<p>
  <b>GPTQ: Accurate Post-training Quantization for Generative Pretrained Transformers</b><br>
  ICLR 2023 - Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh<br>
  <a href="https://arxiv.org/abs/2210.17323">[arxiv]</a> <a href="https://github.com/IST-DASLab/gptq">[github]</a>
  <ul>
    <li>We develop the first quantization algorithm that is fast and accurate enough to successfully quantize 100+ billion parameter models to 4-bit and 3-bit precision.</li>
    <li>We demonstrate for the first time, via custom CUDA kernels, that vanilla weight-only quantization can bring substantial practical speedups for memory-bound generative inference.</li>
  </ul>
</p>

<p>
  <b>Optimal Brain Compression: A Framework for Accurate Pruning and Quantization</b><br>
  NeurIPS 2022 - Elias Frantar, Sidak Pal Singh, Dan Alistarh<br>
  <a href="https://arxiv.org/abs/2208.11580">[arxiv]</a> <a href="https://github.com/IST-DASLab/OBC">[github]</a>
  <ul>
    <li>We show that the classic Optimal Brain Surgeon pruning framework can be implemented *exactly* for layer-wise pruning of modern networks, via new algorithmic techniques, and that it can be extended to perform quantization as well.</li>
    <li>This yields state-of-the-art retraining-free pruning, quantization and mixed pruning & quantization accuracy.</li>
  </ul>
</p>

<p>
  <b>Accurate Pruning with Speedup Guarantees</b><br>
  ICML 2022 - Elias Frantar, Dan Alistarh<br>
  <a href="https://arxiv.org/abs/2201.13096">[arxiv]</a> <a href="https://github.com/IST-DASLab/spdy">[github]</a>
  <ul>
    <li>We introduce a new system for automatically pruning networks to specific real-word inference speedup targets, while minimizing accuracy loss.</li>
    <li>SPDY brings state-to-the-art speedup-vs-accuracy trade-offs in a sparsity-aware CPU inference engine, for both vision and language domains.</li>
  </ul>
</p>

<p>
  <b>M-FAC: Efficient Matrix-Free Approximations of Second-Order Information</b><br>
  NeurIPS 2021 - Elias Frantar, Eldar Kurtic, Dan Alistarh<br>
  <a href="https://arxiv.org/abs/2107.03356">[arxiv]</a> <a href="https://github.com/IST-DASLab/M-FAC">[github]</a>
  <ul>
    <li>We develop the first algorithms for dealing with empirical Fisher approximations that efficiently scale to arbitrarily large blocksizes.</li>
    <li>Those lead to a state-of-the-art pruning algorithm and, supported by efficient CUDA kernels, to a full-matrix second-order optimizer with competitive training and finetuning performance.</li>
  </ul>
</p>
