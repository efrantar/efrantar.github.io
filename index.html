---
layout: home
title: Elias Frantar
subtitle: Large Model Efficiency &amp; Compression
---

I am currently a <b>fourth-year PhD student at ISTAustria</b>, supervised by Dan Alistarh, as well as a <b>second-time intern at Google DeepMind</b>. Overall, my research is focused on making massive machine learning models more efficient.
<br><br>
In 2022, I developed the first successful low-bit quantization and sparsification methods for extremely large language models, <a href=https://arxiv.org/abs/2210.17323>GPTQ</a> and <a href="https://arxiv.org/abs/2301.00774">SparseGPT</a>. In 2023, I identified <a href="https://arxiv.org/abs/2309.08520)">scaling laws for sparsely-connected foundation models</a>, and built <a href="https://arxiv.org/abs/2310.16795">QMoE</a>, a tool for efficient compression and execution of trillion-parameter Mixture-of-Expert models in limited-resource settings. Most recently, I implemented <a href="https://github.com/IST-DASLab/marlin">Marlin</a>, the first INT4xFP16 LLM inference kernel with near-ideal speedup at medium batchsizes.
<br><br>
In my free time, I love creating super fast Rubik's Cube solving robots, some of which have beaten long-standing records and collected millions of views on <a href="https://www.youtube.com/eliasfrantar">YouTube</a>.

<h2 id="highlights">Highlighted Work</h2>

<h4>GPTQ (ICLR 2023):</h4>

<ul>
  <li>The first quantization method able to accurately compress massive LLMs to 4- or 3-bit precision.</li>
  <li>The first open-source GPU kernel demonstrating major generative inference speedup with standard weight-only quantization.</li>
  <li>Supported by various popular libraries: HuggingFace's transformers, NVIDIA's TensorRT-LLM, Intel's neural-compressor.</li>
  <li>1.5k+ stars on <a href="https://github.com/IST-DASLab/gptq">GitHub</a>; with popular forks AutoGPTQ (3k+ stars) and GPTQ-for-LLaMa (2.5k+ stars).</li>
</ul>

<h4>SparseGPT (Oral, ICML 2023):</h4>

<ul>
  <li>The first algorithm able to accurately induce significant sparsity in 100+ billion parameter models.</li>
  <li>Featured by Communications of the ACM and national television.</li>
  <li>Invited talks at Apple, Amazon and Google.</li>
  <li> 500+ stars on <a href="https://github.com/IST-DASLab/sparsegpt">GitHub</a>.</li>
</ul>

<hr>

<h4>Rubik's Cube Robots:</h4>

<ul>
  <li>10+ million views on Youtube; also presented live on BBC.</li>
  <li><a href="https://www.youtube.com/watch?v=Kjb-MmwueEQ">Cuboth</a>: the world's fastest robot to solve an unmodified Rubik's Cube, beating the previous record, which stood for 7 years, by 2x, while using equivalent hardware.</li>
  <li> <a href="https://github.com/efrantar/rob-twophase">rob-twophase</a> &amp; <a href="https://github.com/efrantar/qphase">qphase</a>: the current best computer solving algorithms; also the first to directly take into account robot mechanics during the search process.</li>
  <li> <a href="https://www.youtube.com/watch?v=wLzn1w8vgM4">SquidCuber</a>: the first machine made entirely out of Lego to solve a cube in a single second on average (2x faster than the 5-year-long-standing previous record).</li>
</ul>

<div align="center">
  <a href="https://www.youtube.com/watch?v=Kjb-MmwueEQ"><img src="assets/img/cuboth.jpg" width="275px"></img></a>
  <a href="https://www.youtube.com/watch?v=wLzn1w8vgM4"><img src="assets/img/squidcuber.jpg" width="275px"></img></a>
</div>

<h2 id="papers">First-Author Papers</h2>

<h4>QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models</h4>
<h6><i>MLSys 2023</i> - <u>Elias Frantar</u>, Dan Alistarh - <a href="https://arxiv.org/abs/2310.16795">[arxiv]</a>, <a href="https://github.com/IST-DASLab/qmoe">[github]</a>.</h6>
We compress a trillion-parameter Mixture-of-Experts model to, for the first time, sub-1-bit per parameter, in a custom format co-designed with an efficient inference kernel.

<h4>Scaling Laws for Sparsely-Connected Foundation Models</h4>
<h6><i>SPOTLIGHT, ICLR 2024</i> - <u>E. Frantar</u>, C. Riquelme, N. Houlsby, D. Alistarh, U. Evci - <a href="https://arxiv.org/abs/2309.08520">[arxiv]</a>.</h6>
We determine the first scaling laws connecting parameter-sparsity, effective model size and amount of training data, in the context of modern Transformers trained on massive datasets.</li>

<h4>SparseGPT: Massive Language Models Can be Accurately Pruned in One-shot</h4>
<h6><i>ORAL, ICML 2023</i> - <u>Elias Frantar</u>, Dan Alistarh - <a href="https://arxiv.org/abs/2301.00774">[arxiv]</a>, <a href="https://github.com/IST-DASLab/sparsegpt">[github]</a>.</h6>
We introduce the first pruning algorithm that is fast and accurate enough to successfuly impose non-trivial amounts of sparsity on 100+ billion parameter models.

<h4>GPTQ: Accurate Post-training Quantization for Generative Pretrained Transformers</h4>
<h6><i>ICLR 2023</i> - <u>E. Frantar</u>, S. Ashkboos, T. Hoefler, D. Alistarh - <a href="https://arxiv.org/abs/2210.17323">[arxiv]</a>, <a href="https://github.com/IST-DASLab/gptq">[github]</a>.</h6>
We develop the first quantization algorithm that is fast and accurate enough to successfully quantize 100+ billion parameter models to 4-bit and 3-bit precision.

<h4>Optimal Brain Compression: A Framework for Accurate Pruning and Quantization</h4>
<h6><i>NeurIPS 2022</i> - <u>Elias Frantar</u>, Sidak Pal Singh, Dan Alistarh - <a href="https://arxiv.org/abs/2208.11580">[arxiv]</a>, <a href="https://github.com/IST-DASLab/OBC">[github]</a>.</h6>
We show that the classical Optimal Brain Surgeon pruning framework can be implemented exactly on a layer-wise level and extended to quantization, leading to state-of-the-art post-training compression results.

<h4>Accurate Pruning with Speedup Guarantees</h4>
<h6><i>ICML 2022</i> - <u>Elias Frantar</u>, Dan Alistarh - <a href="https://arxiv.org/abs/2201.13096">[arxiv]</a>, <a href="https://github.com/IST-DASLab/spdy">[github]</a>.</h6>
We introduce new techniques for runtime- and hardware-aware sparsification, with state-of-the-art speedup-vs-accuracy trade-offs, for vision and text domains.

<h4>M-FAC: Efficient Matrix-Free Approximations of Second-Order Information</h4>
<h6><i>NeurIPS 2021</i> - <u>Elias Frantar</u>, Eldar Kurtic, Dan Alistarh - <a href="https://arxiv.org/abs/2107.03356">[arxiv]</a>, <a href="https://github.com/IST-DASLab/M-FAC">[github]</a>.</h6>
We develop new algorithms for dealing with empirical Fisher approximations that efficiently scale to arbitrarily large blocksizes, with applications to pruning and optimization.

<h2>Other Publications</h2>

<h4>SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</h4>
<h6><i>ICLR 2024</i> - T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, <u>E. Frantar</u>, ...</h6>
<hr>
<h4>ZipLM: Hardware-Aware Structured Pruning of Language Models</h4>
<h6><i>NeurIPS 2023</i> - Eldar Kurtic, <u>Elias Frantar</u>, Dan Alistarh</h6>
<hr>
<h4>CAP: Correlation-Aware Pruning for Highly-Accurate Sparse Vision Models</h4>
<h6><i>NeurIPS 2023</i> - Denis Kuznedelev, Eldar Kurtic, <u>Elias Frantar</u>, Dan Alistarh</h6>
<hr>
<h4>The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models</h4>
<h6><i>EMNLP 2022</i> - E. Kurtic, D. Campos, T. Nguyen, <u>E. Frantar</u>, M. Kurtz, B. Fineran, ...</h6>
<hr>
<h4>On the Sample Complexity of Adversarial Multi-Source PAC Learning</h4>
<h6><i>ICML 2020</i> - Nikola Konstantinov, <u>Elias Frantar</u>, Dan Alistarh, Christoph Lampert</h6>
<hr>

<h2 id="contact">Contact</h2>

first-name [dot] last-name [at] (gmail [dot] com OR ist [dot] ac [dot] at)

<div id="links"></div>
